{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_오토인코더_재구축",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPyzHs/ugb9R8GsZryB+sC5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supersfel/AI-Python/blob/main/04_%EC%98%A4%ED%86%A0%EC%9D%B8%EC%BD%94%EB%8D%94_%EC%9E%AC%EA%B5%AC%EC%B6%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwaNdeV0I6qP"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# AutoEncoder를 이용한 MNIST Reconstruction - Keras API를 이용한 구현\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# MNIST 데이터를 다운로드 합니다.\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "# 이미지들을 float32 데이터 타입으로 변경합니다.\n",
        "x_train, x_test = x_train.astype('float32'), x_test.astype('float32')\n",
        "# 28*28 형태의 이미지를 784차원으로 flattening 합니다.\n",
        "x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])\n",
        "# [0, 255] 사이의 값을 [0, 1]사이의 값으로 Normalize합니다.\n",
        "x_train, x_test = x_train / 255., x_test / 255.\n",
        "\n",
        "# 학습에 필요한 설정값들을 정의합니다.\n",
        "learning_rate = 0.02\n",
        "training_epochs = 50    # 반복횟수\n",
        "batch_size = 256        # 배치개수\n",
        "display_step = 1        # 손실함수 출력 주기\n",
        "examples_to_show = 10   # 보여줄 MNIST Reconstruction 이미지 개수\n",
        "input_size = 784        # 28*28\n",
        "hidden1_size = 256 \n",
        "hidden2_size = 128\n",
        "\n",
        "# tf.data API를 이용해서 데이터를 섞고 batch 형태로 가져옵니다.\n",
        "train_data = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "train_data = train_data.shuffle(60000).batch(batch_size)\n",
        "\n",
        "def random_normal_intializer_with_stddev_1():\n",
        "  return tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None)\n",
        "\n",
        "# tf.keras.Model을 이용해서 Autoencoder 모델을 정의합니다.\n",
        "class AutoEncoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(AutoEncoder, self).__init__()\n",
        "    # 인코딩(Encoding) - 784 -> 256 -> 128\n",
        "    self.hidden_layer_1 = tf.keras.layers.Dense(hidden1_size,\n",
        "                                                activation='sigmoid',\n",
        "                                                kernel_initializer=random_normal_intializer_with_stddev_1(),\n",
        "                                                bias_initializer=random_normal_intializer_with_stddev_1())\n",
        "    self.hidden_layer_2 = tf.keras.layers.Dense(hidden2_size,\n",
        "                                                activation='sigmoid',\n",
        "                                                kernel_initializer=random_normal_intializer_with_stddev_1(),\n",
        "                                                bias_initializer=random_normal_intializer_with_stddev_1())\n",
        "    # 디코딩(Decoding) 128 -> 256 -> 784\n",
        "    self.hidden_layer_3 = tf.keras.layers.Dense(hidden1_size,\n",
        "                                                activation='sigmoid',\n",
        "                                                kernel_initializer=random_normal_intializer_with_stddev_1(),\n",
        "                                                bias_initializer=random_normal_intializer_with_stddev_1())\n",
        "    self.output_layer = tf.keras.layers.Dense(input_size,\n",
        "                                                activation='sigmoid',\n",
        "                                                kernel_initializer=random_normal_intializer_with_stddev_1(),\n",
        "                                                bias_initializer=random_normal_intializer_with_stddev_1())\n",
        "\n",
        "  def call(self, x):\n",
        "    H1_output = self.hidden_layer_1(x)\n",
        "    H2_output = self.hidden_layer_2(H1_output)\n",
        "    H3_output = self.hidden_layer_3(H2_output)\n",
        "    reconstructed_x = self.output_layer(H3_output)\n",
        "\n",
        "    return reconstructed_x\n",
        "\n",
        "# MSE 손실 함수를 정의합니다.\n",
        "@tf.function\n",
        "def mse_loss(y_pred, y_true):\n",
        "  return tf.reduce_mean(tf.pow(y_true - y_pred, 2)) # MSE(Mean of Squared Error) 손실함수\n",
        "\n",
        "# 최적화를 위한 RMSProp 옵티마이저를 정의합니다.\n",
        "optimizer = tf.optimizers.RMSprop(learning_rate)\n",
        "\n",
        "# 최적화를 위한 function을 정의합니다.\n",
        "@tf.function\n",
        "def train_step(model, x):\n",
        "  # 타겟데이터는 인풋데이터와 같습니다.\n",
        "  y_true = x\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred = model(x)\n",
        "    loss = mse_loss(y_pred, y_true)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "# Autoencoder 모델을 선언합니다.\n",
        "AutoEncoder_model = AutoEncoder()\n",
        "\n",
        "# 지정된 횟수만큼 최적화를 수행합니다.\n",
        "for epoch in range(training_epochs):\n",
        "  # 모든 배치들에 대해서 최적화를 수행합니다.\n",
        "  # Autoencoder는 Unsupervised Learning이므로 타겟 레이블(label) y가 필요하지 않습니다.\n",
        "  for batch_x in train_data:\n",
        "    # 옵티마이저를 실행해서 파라마터들을 업데이트합니다.\n",
        "    _, current_loss = train_step(AutoEncoder_model, batch_x), mse_loss(AutoEncoder_model(batch_x), batch_x)\n",
        "  # 지정된 epoch마다 학습결과를 출력합니다.\n",
        "  if epoch % display_step == 0:\n",
        "    print(\"반복(Epoch): %d, 손실 함수(Loss): %f\" % ((epoch+1), current_loss))\n",
        "\n",
        "# 테스트 데이터로 Reconstruction을 수행합니다.\n",
        "reconstructed_result = AutoEncoder_model(x_test[:examples_to_show])\n",
        "# 원본 MNIST 데이터와 Reconstruction 결과를 비교합니다.\n",
        "f, a = plt.subplots(2, 10, figsize=(10, 2))\n",
        "for i in range(examples_to_show):\n",
        "  a[0][i].imshow(np.reshape(x_test[i], (28, 28)))\n",
        "  a[1][i].imshow(np.reshape(reconstructed_result[i], (28, 28)))\n",
        "f.savefig('reconstructed_mnist_image.png')  # reconstruction 결과를 png로 저장합니다.\n",
        "f.show()\n",
        "plt.draw()\n",
        "plt.waitforbuttonpress()"
      ]
    }
  ]
}